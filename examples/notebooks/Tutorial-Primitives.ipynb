{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66880e8-23d9-4f00-b120-7f289f682511",
   "metadata": {},
   "source": [
    "# TextGrad Tutorials: Primitives\n",
    "\n",
    "![TextGrad](https://github.com/vinid/data/blob/master/logo_full.png?raw=true)\n",
    "\n",
    "An autograd engine -- for textual gradients!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zou-group/TextGrad/blob/main/examples/notebooks/Prompt-Optimization.ipynb)\n",
    "[![GitHub license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)\n",
    "[![Arxiv](https://img.shields.io/badge/arXiv-2406.07496-B31B1B.svg)](https://arxiv.org/abs/2406.07496)\n",
    "[![Documentation Status](https://readthedocs.org/projects/textgrad/badge/?version=latest)](https://textgrad.readthedocs.io/en/latest/?badge=latest)\n",
    "[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/textgrad)](https://pypi.org/project/textgrad/)\n",
    "[![PyPI](https://img.shields.io/pypi/v/textgrad)](https://pypi.org/project/textgrad/)\n",
    "\n",
    "**Objectives for this tutorial:**\n",
    "\n",
    "* Introduce you to the primitives in TextGrad\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* You need to have an OpenAI API key to run this tutorial. This should be set as an environment variable as OPENAI_API_KEY.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5e8935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/austintimberlake/Desktop/textgrad\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: openai>=1.23.6 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (2.15.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (9.1.2)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (1.2.1)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (2.3.3)\n",
      "Requirement already satisfied: platformdirs>=3.11.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (4.5.1)\n",
      "Requirement already satisfied: datasets>=2.14.6 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (4.4.2)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (5.6.3)\n",
      "Requirement already satisfied: graphviz>=0.20.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (0.21)\n",
      "Requirement already satisfied: gdown>=5.2.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (5.2.1)\n",
      "Requirement already satisfied: litellm>=1.49.5 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (1.80.16)\n",
      "Requirement already satisfied: pillow in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (12.1.0)\n",
      "Requirement already satisfied: httpx in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad==0.1.8) (0.28.1)\n",
      "Requirement already satisfied: filelock in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.6->textgrad==0.1.8) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (1.3.1)\n",
      "Requirement already satisfied: packaging in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad==0.1.8) (6.0.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from gdown>=5.2.0->textgrad==0.1.8) (4.14.3)\n",
      "Requirement already satisfied: anyio in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from httpx->textgrad==0.1.8) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from httpx->textgrad==0.1.8) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from httpx->textgrad==0.1.8) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from httpx->textgrad==0.1.8) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from httpcore==1.*->httpx->textgrad==0.1.8) (0.16.0)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad==0.1.8) (3.13.3)\n",
      "Requirement already satisfied: click in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad==0.1.8) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad==0.1.8) (0.14.0)\n",
      "Requirement already satisfied: grpcio!=1.68.*,!=1.69.*,!=1.70.*,!=1.71.0,!=1.71.1,!=1.72.0,!=1.72.1,!=1.73.0,>=1.62.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad==0.1.8) (1.76.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad==0.1.8) (8.7.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad==0.1.8) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.23.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad==0.1.8) (4.26.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad==0.1.8) (2.12.5)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad==0.1.8) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad==0.1.8) (0.22.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from openai>=1.23.6->textgrad==0.1.8) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from openai>=1.23.6->textgrad==0.1.8) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from openai>=1.23.6->textgrad==0.1.8) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from openai>=1.23.6->textgrad==0.1.8) (4.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pandas>=1.5.3->textgrad==0.1.8) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pandas>=1.5.3->textgrad==0.1.8) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pandas>=1.5.3->textgrad==0.1.8) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad==0.1.8) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad==0.1.8) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad==0.1.8) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad==0.1.8) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad==0.1.8) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad==0.1.8) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad==0.1.8) (1.22.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.6->textgrad==0.1.8) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.6->textgrad==0.1.8) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.6->textgrad==0.1.8) (0.21.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm>=1.49.5->textgrad==0.1.8) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.49.5->textgrad==0.1.8) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm>=1.49.5->textgrad==0.1.8) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm>=1.49.5->textgrad==0.1.8) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm>=1.49.5->textgrad==0.1.8) (0.30.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.49.5->textgrad==0.1.8) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.49.5->textgrad==0.1.8) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.49.5->textgrad==0.1.8) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.3->textgrad==0.1.8) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.14.6->textgrad==0.1.8) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.14.6->textgrad==0.1.8) (2.6.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm>=1.49.5->textgrad==0.1.8) (2025.11.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from beautifulsoup4->gdown>=5.2.0->textgrad==0.1.8) (2.8.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from requests[socks]->gdown>=5.2.0->textgrad==0.1.8) (1.7.1)\n",
      "Building wheels for collected packages: textgrad\n",
      "  Building editable for textgrad (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for textgrad: filename=textgrad-0.1.8-0.editable-py3-none-any.whl size=9866 sha256=4dd67bdbe7dd0c5d87aa9e5aa56280d3f55100b8a62f3821de6b8b063cb31009\n",
      "  Stored in directory: /private/var/folders/fd/ntt6v6ts64x_yhx3c3054zv00000gn/T/pip-ephem-wheel-cache-43d7hkot/wheels/31/6c/b3/1bcc883beb125269ce5253459fb3951302e0ab797bda886328\n",
      "Successfully built textgrad\n",
      "Installing collected packages: textgrad\n",
      "  Attempting uninstall: textgrad\n",
      "    Found existing installation: textgrad 0.1.8\n",
      "    Uninstalling textgrad-0.1.8:\n",
      "      Successfully uninstalled textgrad-0.1.8\n",
      "Successfully installed textgrad-0.1.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a9c615-17a0-455c-8f9c-f0d25fb8824b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:43:10.594204491Z",
     "start_time": "2024-06-11T15:43:10.589328053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textgrad in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (0.1.8)\n",
      "Requirement already satisfied: openai>=1.23.6 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (2.15.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (9.1.2)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (1.2.1)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (2.3.3)\n",
      "Requirement already satisfied: platformdirs>=3.11.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (4.5.1)\n",
      "Requirement already satisfied: datasets>=2.14.6 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (4.4.2)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (5.6.3)\n",
      "Requirement already satisfied: graphviz>=0.20.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (0.21)\n",
      "Requirement already satisfied: gdown>=5.2.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (5.2.1)\n",
      "Requirement already satisfied: litellm>=1.49.5 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (1.80.16)\n",
      "Requirement already satisfied: pillow in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (12.1.0)\n",
      "Requirement already satisfied: httpx in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from textgrad) (0.28.1)\n",
      "Requirement already satisfied: filelock in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.6->textgrad) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (1.3.1)\n",
      "Requirement already satisfied: packaging in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (6.0.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from gdown>=5.2.0->textgrad) (4.14.3)\n",
      "Requirement already satisfied: anyio in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from httpx->textgrad) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from httpx->textgrad) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from httpx->textgrad) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from httpx->textgrad) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from httpcore==1.*->httpx->textgrad) (0.16.0)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad) (3.13.3)\n",
      "Requirement already satisfied: click in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad) (0.14.0)\n",
      "Requirement already satisfied: grpcio!=1.68.*,!=1.69.*,!=1.70.*,!=1.71.0,!=1.71.1,!=1.72.0,!=1.72.1,!=1.73.0,>=1.62.3 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad) (1.76.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad) (8.7.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.23.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad) (4.26.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad) (2.12.5)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from litellm>=1.49.5->textgrad) (0.22.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from openai>=1.23.6->textgrad) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from openai>=1.23.6->textgrad) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from openai>=1.23.6->textgrad) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from openai>=1.23.6->textgrad) (4.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pandas>=1.5.3->textgrad) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pandas>=1.5.3->textgrad) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pandas>=1.5.3->textgrad) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (1.22.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.6->textgrad) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.6->textgrad) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.6->textgrad) (0.21.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm>=1.49.5->textgrad) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.49.5->textgrad) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm>=1.49.5->textgrad) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm>=1.49.5->textgrad) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm>=1.49.5->textgrad) (0.30.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.49.5->textgrad) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.49.5->textgrad) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.49.5->textgrad) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.3->textgrad) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.14.6->textgrad) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.14.6->textgrad) (2.6.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm>=1.49.5->textgrad) (2025.11.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from beautifulsoup4->gdown>=5.2.0->textgrad) (2.8.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/austintimberlake/Desktop/textgrad/.venv312/lib/python3.12/site-packages (from requests[socks]->gdown>=5.2.0->textgrad) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install textgrad # you might need to restart the notebook after installing textgrad\n",
    "\n",
    "from textgrad.engine import get_engine\n",
    "from textgrad import Variable\n",
    "from textgrad.optimizer import TextualGradientDescent\n",
    "from textgrad.loss import TextLoss\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887fbed36c7daf2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Introduction: Variable\n",
    "\n",
    "Variables in TextGrad are the metaphorical equivalent of tensors in PyTorch. They are the primary data structure that you will interact with when using TextGrad. \n",
    "\n",
    "Variables keep track of gradients and manage the data.\n",
    "\n",
    "Variables require two arguments (and there is an optional third one):\n",
    "\n",
    "1. `data`: The data that the variable will hold\n",
    "2. `role_description`: A description of the role of the variable in the computation graph\n",
    "3. `requires_grad`: (optional) A boolean flag that indicates whether the variable requires gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65fb4456d84c8fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:43:17.669096228Z",
     "start_time": "2024-06-11T15:43:17.665325560Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = Variable(\"A sntence with a typo\", role_description=\"The input sentence\", requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65857dd50408ebd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:43:18.184004948Z",
     "start_time": "2024-06-11T15:43:18.178187640Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6a6921a1cce6a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Introduction: Engine\n",
    "\n",
    "When we talk about the engine in TextGrad, we are referring to an LLM. The engine is an abstraction we use to interact with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281644022ac1c65d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:44:32.606319032Z",
     "start_time": "2024-06-11T15:44:32.561460448Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "engine = get_engine(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53a561a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sentence \"The quick brown fox jumps over the lazy dog.\" is correct. It is a pangram, meaning it contains every letter of the English alphabet at least once.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT-5 mini with a custom system prompt and input\n",
    "engine = get_engine(\"gpt-4o-mini\")\n",
    "custom_system_prompt = \"Evaluate the correctness of this sentence\"\n",
    "custom_input = \"The quick brown fox jumps over the lazy dog.\"\n",
    "engine.generate(custom_input, system_prompt=custom_system_prompt, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7d6eaa115cd6a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This object behaves like you would expect an LLM to behave: You can sample generation from the engine using the `generate` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37502bf67ef23c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T17:29:41.108552705Z",
     "start_time": "2024-06-11T17:29:40.294256814Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.generate(\"Hello how are you?\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627edc07c0d3737",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Introduction: Loss\n",
    "\n",
    "Again, Loss in TextGrad is the metaphorical equivalent of loss in PyTorch. We use Losses in different form in TextGrad but for now we will focus on a simple TextLoss. TextLoss is going to evaluate the loss wrt a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252e0a0152b81f14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:44:32.894722136Z",
     "start_time": "2024-06-11T15:44:32.890708561Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = Variable(\"Evaluate the correctness of this sentence\", role_description=\"The system prompt\")\n",
    "loss = TextLoss(system_prompt, engine=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff137c99e0659dcc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f05ec2bf907b3ba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Introduction: Optimizer\n",
    "\n",
    "Keeping on the analogy with PyTorch, the optimizer in TextGrad is the object that will update the parameters of the model. In this case, the parameters are the variables that have `requires_grad` set to `True`.\n",
    "\n",
    "**NOTE** This is a text optimizer! It will do all operations with text! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f93f80b9e3ad36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:44:33.741130951Z",
     "start_time": "2024-06-11T15:44:33.734977769Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = TextualGradientDescent(parameters=[x], engine=engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26883eb74ce0d01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Putting it all together\n",
    "\n",
    "We can now put all the pieces together. We have a variable, an engine, a loss, and an optimizer. We can now run a single optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9817e0ae0179376d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:44:41.730132530Z",
     "start_time": "2024-06-11T15:44:34.997777872Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "l = loss(x)\n",
    "l.backward(engine)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e3fab0efdd579e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:44:41.738985151Z",
     "start_time": "2024-06-11T15:44:41.731989729Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notice the typo in this sentence.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8aab93b80fb82c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "While here it is not going to be useful, we can also do multiple optimization steps in a loop! Do not forget to reset the gradients after each step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6bb8d0dcc2539d1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-11T15:44:30.989940227Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a84aad4cd58737",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e16a0ae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     63\u001b[39m loss_input = F.sum(\n\u001b[32m     64\u001b[39m     [\n\u001b[32m     65\u001b[39m         Variable(\u001b[33m\"\u001b[39m\u001b[33mSentence:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, role_description=\u001b[33m\"\u001b[39m\u001b[33mloss prefix\u001b[39m\u001b[33m\"\u001b[39m, requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m     ]\n\u001b[32m     70\u001b[39m )\n\u001b[32m     72\u001b[39m l = loss(loss_input)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m optimizer.step()\n\u001b[32m     75\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/textgrad/variable.py:189\u001b[39m, in \u001b[36mVariable.backward\u001b[39m\u001b[34m(self, engine)\u001b[39m\n\u001b[32m    187\u001b[39m v.gradients = _check_and_reduce_gradients(v, backward_engine)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m v.get_grad_fn() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_engine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackward_engine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/textgrad/autograd/function.py:57\u001b[39m, in \u001b[36mBackwardContext.__call__\u001b[39m\u001b[34m(self, backward_engine)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, backward_engine: EngineLM):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward_engine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackward_engine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/textgrad/autograd/llm_ops.py:96\u001b[39m, in \u001b[36mLLMCall.backward\u001b[39m\u001b[34m(self, response, prompt, system_prompt, backward_engine)\u001b[39m\n\u001b[32m     94\u001b[39m children_variables = response.predecessors\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.get_gradient_text() == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backward_through_llm_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchildren_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward_engine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mself\u001b[39m._backward_through_llm_chain(children_variables, response, prompt, system_prompt, backward_engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/textgrad/autograd/llm_ops.py:210\u001b[39m, in \u001b[36mLLMCall._backward_through_llm_base\u001b[39m\u001b[34m(variables, response, prompt, system_prompt, backward_engine)\u001b[39m\n\u001b[32m    207\u001b[39m backward_prompt = LLMCall._construct_llm_base_backward_prompt(backward_info)\n\u001b[32m    209\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m_backward_through_llm prompt\u001b[39m\u001b[33m\"\u001b[39m, extra={\u001b[33m\"\u001b[39m\u001b[33m_backward_through_llm\u001b[39m\u001b[33m\"\u001b[39m: backward_prompt})\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m gradient_value = \u001b[43mbackward_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBACKWARD_SYSTEM_PROMPT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m_backward_through_llm gradient\u001b[39m\u001b[33m\"\u001b[39m, extra={\u001b[33m\"\u001b[39m\u001b[33m_backward_through_llm\u001b[39m\u001b[33m\"\u001b[39m: gradient_value})\n\u001b[32m    213\u001b[39m conversation = CONVERSATION_TEMPLATE.format(**backward_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/textgrad/engine/openai.py:111\u001b[39m, in \u001b[36mBaseOpenAIEngine.__call__\u001b[39m\u001b[34m(self, prompt, **kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/textgrad/engine/openai.py:54\u001b[39m, in \u001b[36mBaseOpenAIEngine.generate\u001b[39m\u001b[34m(self, content, system_prompt, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;129m@retry\u001b[39m(wait=wait_random_exponential(\u001b[38;5;28mmin\u001b[39m=\u001b[32m1\u001b[39m, \u001b[38;5;28mmax\u001b[39m=\u001b[32m5\u001b[39m), stop=stop_after_attempt(\u001b[32m5\u001b[39m))\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m     48\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     **kwargs,\n\u001b[32m     52\u001b[39m ):\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_from_single_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     59\u001b[39m         has_multimodal_input = \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/textgrad/engine/openai.py:104\u001b[39m, in \u001b[36mBaseOpenAIEngine._generate_from_single_prompt\u001b[39m\u001b[34m(self, prompt, system_prompt, temperature, max_tokens, top_p)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    102\u001b[39m     request_params[\u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m] = max_tokens\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m response = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m    107\u001b[39m \u001b[38;5;28mself\u001b[39m._save_cache(sys_prompt_arg + prompt, response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/textgrad/.venv312/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Chain an LLM call into a TextGrad optimization step\n",
    "# with a numeric blackbox judge in the loop.\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "from textgrad import BlackboxLLM\n",
    "from textgrad.autograd import functional as F\n",
    "\n",
    "polite_system_prompt = Variable(\n",
    "    \"You are a helpful assistant. Respond with a short sentence.\",\n",
    "    role_description=\"System prompt for generating a sentence\",\n",
    "    requires_grad=True,\n",
    ")\n",
    "\n",
    "generate_sentence_prompt = Variable(\n",
    "    f\"Try to say something to get out of an awkward situation. Output only the sentence you want to say. Nonce: {uuid.uuid4()}\",\n",
    "    role_description=\"User prompt for sentence generation\",\n",
    "    requires_grad=False,\n",
    ")\n",
    "\n",
    "model = BlackboxLLM(engine=engine, system_prompt=polite_system_prompt)\n",
    "raw_sentence = model(generate_sentence_prompt)\n",
    "\n",
    "judge_engine = get_engine(\"gpt-4o-mini\")\n",
    "\n",
    "def judge_politeness_score(text: str) -> int:\n",
    "    judge_prompt = (\n",
    "        \"You are a strict politeness judge. Rate the politeness of the sentence from 1 to 10. \"\n",
    "        \"Respond with only a single integer from 1 to 10.\"\n",
    "    )\n",
    "    response = judge_engine.generate(\n",
    "        f\"{judge_prompt}\\nSentence: {text}\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    match = re.search(r\"\\d+\", response)\n",
    "    if not match:\n",
    "        return 1\n",
    "    score = int(match.group(0))\n",
    "    return max(1, min(10, score))\n",
    "\n",
    "# Use a fixed evaluation prompt and optimize the generation system prompt.\n",
    "# The blackbox judge only provides a numeric score; TextGrad uses it as context.\n",
    "eval_system_prompt = Variable(\n",
    "    \"You will receive a sentence and a politeness score from a separate blackbox judge. \"\n",
    "    \"Explain how to improve the sentence to increase the score toward 10. Be concise and actionable.\",\n",
    "    role_description=\"The system prompt\",\n",
    "    requires_grad=False,\n",
    ")\n",
    "loss = TextLoss(eval_system_prompt, engine=engine)\n",
    "optimizer = TextualGradientDescent(parameters=[polite_system_prompt], engine=engine)\n",
    "\n",
    "for step in range(20):\n",
    "    # Re-generate first so the loss is computed on the current prompt.\n",
    "    raw_sentence = model(generate_sentence_prompt)\n",
    "\n",
    "    score = judge_politeness_score(raw_sentence.value)\n",
    "    score_var = Variable(\n",
    "        f\"Politeness score (1-10) from blackbox judge: {score}\",\n",
    "        role_description=\"numeric politeness score\",\n",
    "        requires_grad=False,\n",
    "    )\n",
    "\n",
    "    loss_input = F.sum(\n",
    "        [\n",
    "            Variable(\"Sentence:\\n\", role_description=\"loss prefix\", requires_grad=False),\n",
    "            raw_sentence,\n",
    "            Variable(\"\\n\", role_description=\"loss separator\", requires_grad=False),\n",
    "            score_var,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    l = loss(loss_input)\n",
    "    l.backward(engine)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Step {step + 1} system prompt:\")\n",
    "    print(polite_system_prompt.value)\n",
    "    print(\"LLM output:\")\n",
    "    print(raw_sentence.value)\n",
    "    print(f\"Blackbox score: {score}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6d7995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rich in /Users/brian/miniconda3/lib/python3.13/site-packages (13.9.4)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting toml\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting transformer-lens\n",
      "  Downloading transformer_lens-2.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/brian/miniconda3/lib/python3.13/site-packages (from rich) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/brian/miniconda3/lib/python3.13/site-packages (from rich) (2.19.1)\n",
      "Collecting numpy>=1.24.1 (from scikit-learn)\n",
      "  Downloading numpy-2.4.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.17.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate>=0.23.0 (from transformer-lens)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting beartype<0.15.0,>=0.14.1 (from transformer-lens)\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting better-abc<0.0.4,>=0.0.3 (from transformer-lens)\n",
      "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting datasets>=2.7.1 (from transformer-lens)\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting einops>=0.6.0 (from transformer-lens)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fancy-einsum>=0.0.3 (from transformer-lens)\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jaxtyping>=0.2.11 (from transformer-lens)\n",
      "  Downloading jaxtyping-0.3.5-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pandas>=1.1.5 (from transformer-lens)\n",
      "  Downloading pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting sentencepiece (from transformer-lens)\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting torch>=2.6 (from transformer-lens)\n",
      "  Downloading torch-2.9.1-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/brian/miniconda3/lib/python3.13/site-packages (from transformer-lens) (4.67.1)\n",
      "Collecting transformers>=4.51 (from transformer-lens)\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer-lens)\n",
      "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typeguard<5.0,>=4.2 (from transformer-lens)\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/brian/miniconda3/lib/python3.13/site-packages (from transformer-lens) (4.12.2)\n",
      "Collecting wandb>=0.13.5 (from transformer-lens)\n",
      "  Downloading wandb-0.24.0-py3-none-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/brian/miniconda3/lib/python3.13/site-packages (from accelerate>=0.23.0->transformer-lens) (24.2)\n",
      "Requirement already satisfied: psutil in /Users/brian/miniconda3/lib/python3.13/site-packages (from accelerate>=0.23.0->transformer-lens) (7.2.1)\n",
      "Collecting pyyaml (from accelerate>=0.23.0->transformer-lens)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=0.23.0->transformer-lens)\n",
      "  Downloading huggingface_hub-1.3.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.23.0->transformer-lens)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting filelock (from datasets>=2.7.1->transformer-lens)\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=2.7.1->transformer-lens)\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets>=2.7.1->transformer-lens)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/brian/miniconda3/lib/python3.13/site-packages (from datasets>=2.7.1->transformer-lens) (2.32.3)\n",
      "Collecting httpx<1.0.0 (from datasets>=2.7.1->transformer-lens)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting xxhash (from datasets>=2.7.1->transformer-lens)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets>=2.7.1->transformer-lens)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer-lens)\n",
      "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/brian/miniconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/brian/miniconda3/lib/python3.13/site-packages (from pandas>=1.1.5->transformer-lens) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.5->transformer-lens)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.5->transformer-lens)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /Users/brian/miniconda3/lib/python3.13/site-packages (from torch>=2.6->transformer-lens) (78.1.1)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.6->transformer-lens)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=2.6->transformer-lens)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch>=2.6->transformer-lens)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=0.23.0->transformer-lens)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.51->transformer-lens)\n",
      "  Downloading regex-2026.1.15-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.51->transformer-lens)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-extensions (from transformer-lens)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting click>=8.0.1 (from wandb>=0.13.5->transformer-lens)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading gitpython-3.1.46-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /Users/brian/miniconda3/lib/python3.13/site-packages (from wandb>=0.13.5->transformer-lens) (4.3.7)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading protobuf-6.33.4-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic<3 in /Users/brian/miniconda3/lib/python3.13/site-packages (from wandb>=0.13.5->transformer-lens) (2.10.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.13.5->transformer-lens)\n",
      "  Downloading sentry_sdk-2.49.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens)\n",
      "  Downloading aiohttp-3.13.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets>=2.7.1->transformer-lens)\n",
      "  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in /Users/brian/miniconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.7.1->transformer-lens) (2026.1.4)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets>=2.7.1->transformer-lens)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /Users/brian/miniconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.7.1->transformer-lens) (3.7)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets>=2.7.1->transformer-lens)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.21.0->accelerate>=0.23.0->transformer-lens)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/brian/miniconda3/lib/python3.13/site-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/brian/miniconda3/lib/python3.13/site-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/brian/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer-lens) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/brian/miniconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/brian/miniconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens) (2.3.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.6->transformer-lens)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/brian/miniconda3/lib/python3.13/site-packages (from jinja2->torch>=2.6->transformer-lens) (3.0.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens)\n",
      "  Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens)\n",
      "  Downloading multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens)\n",
      "  Downloading propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.7.1->transformer-lens)\n",
      "  Downloading yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading transformer_lens-2.16.1-py3-none-any.whl (192 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
      "Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading jaxtyping-0.3.5-py3-none-any.whl (55 kB)\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading numpy-2.4.1-cp313-cp313-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.17.0-cp313-cp313-macosx_14_0_arm64.whl (20.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading torch-2.9.1-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading wandb-0.24.0-py3-none-macosx_12_0_arm64.whl (21.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.1-cp313-cp313-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading gitpython-3.1.46-py3-none-any.whl (208 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.4-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-macosx_12_0_arm64.whl (34.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2026.1.15-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Downloading sentry_sdk-2.49.0-py2.py3-none-any.whl (415 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
      "Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohttp-3.13.3-cp313-cp313-macosx_11_0_arm64.whl (490 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl (49 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl (46 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl (93 kB)\n",
      "Building wheels for collected packages: transformers-stream-generator\n",
      "  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12527 sha256=dd941885b406b45730bd1abc1b9dc727acd5f2ef9bcd14eb0baf8ae5646c1053\n",
      "  Stored in directory: /Users/brian/Library/Caches/pip/wheels/f6/6e/1a/710143d0e50e827ae5b60a47a6d43d715cf1c6e35881a05530\n",
      "Successfully built transformers-stream-generator\n",
      "Installing collected packages: pytz, mpmath, better-abc, xxhash, wadler-lindig, tzdata, typing-extensions, toml, threadpoolctl, sympy, smmap, sentry-sdk, sentencepiece, safetensors, regex, pyyaml, python-dotenv, pyarrow, protobuf, propcache, numpy, networkx, multidict, joblib, jinja2, hf-xet, h11, fsspec, frozenlist, filelock, fancy-einsum, einops, dill, click, beartype, attrs, anyio, aiohappyeyeballs, yarl, typeguard, torch, scipy, pandas, multiprocess, jaxtyping, huggingface_hub, httpcore, gitdb, aiosignal, tokenizers, scikit-learn, httpx, gitpython, aiohttp, accelerate, wandb, transformers, transformers-stream-generator, datasets, transformer-lens\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "Successfully installed accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 anyio-4.12.1 attrs-25.4.0 beartype-0.14.1 better-abc-0.0.3 click-8.3.1 datasets-4.5.0 dill-0.4.0 einops-0.8.1 fancy-einsum-0.0.3 filelock-3.20.3 frozenlist-1.8.0 fsspec-2025.10.0 gitdb-4.0.12 gitpython-3.1.46 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface_hub-0.36.0 jaxtyping-0.3.5 jinja2-3.1.6 joblib-1.5.3 mpmath-1.3.0 multidict-6.7.0 multiprocess-0.70.18 networkx-3.6.1 numpy-2.4.1 pandas-2.3.3 propcache-0.4.1 protobuf-6.33.4 pyarrow-22.0.0 python-dotenv-1.2.1 pytz-2025.2 pyyaml-6.0.3 regex-2026.1.15 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.17.0 sentencepiece-0.2.1 sentry-sdk-2.49.0 smmap-5.0.2 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.2 toml-0.10.2 torch-2.9.1 transformer-lens-2.16.1 transformers-4.57.6 transformers-stream-generator-0.0.5 typeguard-4.4.4 typing-extensions-4.15.0 tzdata-2025.3 wadler-lindig-0.1.7 wandb-0.24.0 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the subspace-rerouting submodule to the path (relative to this notebook)\n",
    "# Notebook is in examples/notebooks/, submodule is in external/subspace-rerouting\n",
    "submodule_path = os.path.abspath(os.path.join(os.getcwd(), '../../external/subspace-rerouting'))\n",
    "sys.path.append(submodule_path)\n",
    "\n",
    "# Install required dependencies for the subspace-rerouting submodule\n",
    "%pip install rich python-dotenv scikit-learn toml transformer-lens\n",
    "\n",
    "from ssr.lens import Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e21b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Authenticated with Hugging Face using token from .env file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">40</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 37 # Initialize Lens with a preset model (e.g., llama3.2_1b)</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 38 # ============================================================================</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 39 # Available presets: \"llama3.2_1b\", \"llama3.2_3b\", \"gemma2_2b\", \"qwen2.5_1.5b\"</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 40 lens = <span style=\"font-weight: bold; text-decoration: underline\">Lens.from_preset(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"llama3.2_1b\"</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 41 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 42 # Define your user message (no system prompt)</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 43 </span>user_message = <span style=\"color: #808000; text-decoration-color: #808000\">\"What is the capital of France?\"</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/brian/Desktop/TextGrad-clone/external/subspace-rerouting/ssr/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">lens.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">140</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_preset</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>device=default_values.device,                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">138 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>dtype=<span style=\"color: #808000; text-decoration-color: #808000\">\"float16\"</span>,                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>center_unembed=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>140 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>cent<span style=\"font-weight: bold; text-decoration: underline\">er_writing_weights=</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">True</span><span style=\"font-weight: bold; text-decoration: underline\">,</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">141 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">            </span><span style=\"font-weight: bold; text-decoration: underline\">fold_ln=</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">True</span><span style=\"font-weight: bold; text-decoration: underline\">,</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">142 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">         </span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">143 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">else</span><span style=\"font-weight: bold; text-decoration: underline\">:</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/brian/miniconda3/lib/python3.13/site-packages/transformer_lens/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">HookedTransformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1417</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained_no_processing</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1414 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">      </span><span style=\"color: #808000; text-decoration-color: #808000\">Wrapper for from_pretrained with all boolean flags related to simplifying the mo</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1415 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">      </span><span style=\"color: #808000; text-decoration-color: #808000\">False. Refer to from_pretrained for details.</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1416 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">      </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1417 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>.from_pretrained(                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1418 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>model_name,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1419 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>fold_ln=fold_ln,                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1420 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>center_writing_weights=center_writing_weights,                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/brian/miniconda3/lib/python3.13/site-packages/transformer_lens/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">HookedTransformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1370</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1367 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1368 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Get the state dict of the model (ie a mapping of parameter names to tensors), </span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1369 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># match the HookedTransformer parameter names.</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1370 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>state_dict = <span style=\"font-weight: bold; text-decoration: underline\">loading.get_pretrained_state_dict(</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1371 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">         </span><span style=\"font-weight: bold; text-decoration: underline\">official_model_name, cfg, hf_model, dtype=dtype, **from_pretrained_kwargs</span>     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1372 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">      </span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1373 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/brian/miniconda3/lib/python3.13/site-packages/transformer_lens/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loading_from_pretrained.py</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1962</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_pretrained_state_dict</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1959 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> cfg.original_architecture == <span style=\"color: #808000; text-decoration-color: #808000\">\"GPTNeoXForCausalLM\"</span>:                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1960 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>state_dict = convert_neox_weights(hf_model, cfg)                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1961 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> cfg.original_architecture == <span style=\"color: #808000; text-decoration-color: #808000\">\"LlamaForCausalLM\"</span>:                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1962 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>state_dict = <span style=\"font-weight: bold; text-decoration: underline\">convert_llama_weights(hf_model, cfg)</span>                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1963 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> cfg.original_architecture == <span style=\"color: #808000; text-decoration-color: #808000\">\"BertForMaskedLM\"</span>:                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1964 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>state_dict = convert_bert_weights(hf_model, cfg)                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1965 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> cfg.original_architecture == <span style=\"color: #808000; text-decoration-color: #808000\">\"T5ForConditionalGeneration\"</span>:                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/brian/miniconda3/lib/python3.13/site-packages/transformer_lens/pretrained/weight_conversi</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">ons/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">44</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert_llama_weights</span>                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>state_dict[<span style=\"color: #808000; text-decoration-color: #808000\">f\"blocks.{</span>l<span style=\"color: #808000; text-decoration-color: #808000\">}.attn.{</span>gqa_uscore<span style=\"color: #808000; text-decoration-color: #808000\">}W_K\"</span>] = W_K                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">42 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>state_dict[<span style=\"color: #808000; text-decoration-color: #808000\">f\"blocks.{</span>l<span style=\"color: #808000; text-decoration-color: #808000\">}.attn.{</span>gqa_uscore<span style=\"color: #808000; text-decoration-color: #808000\">}W_V\"</span>] = W_V                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">43 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>44 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>state_dict[<span style=\"color: #808000; text-decoration-color: #808000\">f\"blocks.{</span>l<span style=\"color: #808000; text-decoration-color: #808000\">}.attn.b_Q\"</span>] = <span style=\"font-weight: bold; text-decoration: underline\">torch.zeros(</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">         </span><span style=\"font-weight: bold; text-decoration: underline\">cfg.n_heads, cfg.d_head, dtype=cfg.dtype, device=cfg.device</span>                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold; text-decoration: underline\">      </span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>state_dict[<span style=\"color: #808000; text-decoration-color: #808000\">f\"blocks.{</span>l<span style=\"color: #808000; text-decoration-color: #808000\">}.attn.{</span>gqa_uscore<span style=\"color: #808000; text-decoration-color: #808000\">}b_K\"</span>] = torch.zeros(                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/brian/miniconda3/lib/python3.13/site-packages/torch/cuda/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">403</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_lazy_init</span>    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 400 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">\"multiprocessing, you must use the 'spawn' start method\"</span>                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 401 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 402 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">hasattr</span>(torch._C, <span style=\"color: #808000; text-decoration-color: #808000\">\"_cuda_getDeviceCount\"</span>):                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 403 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">AssertionError</span><span style=\"font-weight: bold; text-decoration: underline\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"Torch not compiled with CUDA enabled\"</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 404 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _cudart <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 405 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">AssertionError</span>(                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 406 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">\"libcudart functions unavailable. It looks like you have a broken build?</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError: </span>Torch not compiled with CUDA enabled\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m40\u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 37 \u001b[0m\u001b[2m# Initialize Lens with a preset model (e.g., llama3.2_1b)\u001b[0m                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 38 \u001b[0m\u001b[2m# ============================================================================\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 39 \u001b[0m\u001b[2m# Available presets: \"llama3.2_1b\", \"llama3.2_3b\", \"gemma2_2b\", \"qwen2.5_1.5b\"\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 40 lens = \u001b[1;4mLens.from_preset(\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mllama3.2_1b\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m)\u001b[0m                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 41 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 42 \u001b[0m\u001b[2m# Define your user message (no system prompt)\u001b[0m                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 43 \u001b[0muser_message = \u001b[33m\"\u001b[0m\u001b[33mWhat is the capital of France?\u001b[0m\u001b[33m\"\u001b[0m                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/Users/brian/Desktop/TextGrad-clone/external/subspace-rerouting/ssr/\u001b[0m\u001b[1;33mlens.py\u001b[0m:\u001b[94m140\u001b[0m in \u001b[92mfrom_preset\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m            \u001b[0mdevice=default_values.device,                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m            \u001b[0mdtype=\u001b[33m\"\u001b[0m\u001b[33mfloat16\u001b[0m\u001b[33m\"\u001b[0m,                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m            \u001b[0mcenter_unembed=\u001b[94mTrue\u001b[0m,                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m140 \u001b[2m            \u001b[0mcent\u001b[1;4mer_writing_weights=\u001b[0m\u001b[1;4;94mTrue\u001b[0m\u001b[1;4m,\u001b[0m                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m141 \u001b[0m\u001b[1;2;4m            \u001b[0m\u001b[1;4mfold_ln=\u001b[0m\u001b[1;4;94mTrue\u001b[0m\u001b[1;4m,\u001b[0m                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m142 \u001b[0m\u001b[1;2;4m         \u001b[0m\u001b[1;4m)\u001b[0m                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m143 \u001b[0m\u001b[1;2;4m      \u001b[0m\u001b[1;4;94melse\u001b[0m\u001b[1;4m:\u001b[0m                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/Users/brian/miniconda3/lib/python3.13/site-packages/transformer_lens/\u001b[0m\u001b[1;33mHookedTransformer.py\u001b[0m:\u001b[94m1417\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mfrom_pretrained_no_processing\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1414 \u001b[0m\u001b[2;33m      \u001b[0m\u001b[33mWrapper for from_pretrained with all boolean flags related to simplifying the mo\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1415 \u001b[0m\u001b[2;33m      \u001b[0m\u001b[33mFalse. Refer to from_pretrained for details.\u001b[0m                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1416 \u001b[0m\u001b[2;33m      \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1417 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mcls\u001b[0m.from_pretrained(                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1418 \u001b[0m\u001b[2m         \u001b[0mmodel_name,                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1419 \u001b[0m\u001b[2m         \u001b[0mfold_ln=fold_ln,                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1420 \u001b[0m\u001b[2m         \u001b[0mcenter_writing_weights=center_writing_weights,                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/Users/brian/miniconda3/lib/python3.13/site-packages/transformer_lens/\u001b[0m\u001b[1;33mHookedTransformer.py\u001b[0m:\u001b[94m1370\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1367 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1368 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Get the state dict of the model (ie a mapping of parameter names to tensors), \u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1369 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# match the HookedTransformer parameter names.\u001b[0m                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1370 \u001b[2m      \u001b[0mstate_dict = \u001b[1;4mloading.get_pretrained_state_dict(\u001b[0m                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1371 \u001b[0m\u001b[1;2;4m         \u001b[0m\u001b[1;4mofficial_model_name, cfg, hf_model, dtype=dtype, **from_pretrained_kwargs\u001b[0m     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1372 \u001b[0m\u001b[1;2;4m      \u001b[0m\u001b[1;4m)\u001b[0m                                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1373 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/Users/brian/miniconda3/lib/python3.13/site-packages/transformer_lens/\u001b[0m\u001b[1;33mloading_from_pretrained.py\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m :\u001b[94m1962\u001b[0m in \u001b[92mget_pretrained_state_dict\u001b[0m                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1959 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melif\u001b[0m cfg.original_architecture == \u001b[33m\"\u001b[0m\u001b[33mGPTNeoXForCausalLM\u001b[0m\u001b[33m\"\u001b[0m:                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1960 \u001b[0m\u001b[2m         \u001b[0mstate_dict = convert_neox_weights(hf_model, cfg)                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1961 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melif\u001b[0m cfg.original_architecture == \u001b[33m\"\u001b[0m\u001b[33mLlamaForCausalLM\u001b[0m\u001b[33m\"\u001b[0m:                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1962 \u001b[2m         \u001b[0mstate_dict = \u001b[1;4mconvert_llama_weights(hf_model, cfg)\u001b[0m                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1963 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melif\u001b[0m cfg.original_architecture == \u001b[33m\"\u001b[0m\u001b[33mBertForMaskedLM\u001b[0m\u001b[33m\"\u001b[0m:                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1964 \u001b[0m\u001b[2m         \u001b[0mstate_dict = convert_bert_weights(hf_model, cfg)                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1965 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melif\u001b[0m cfg.original_architecture == \u001b[33m\"\u001b[0m\u001b[33mT5ForConditionalGeneration\u001b[0m\u001b[33m\"\u001b[0m:                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/Users/brian/miniconda3/lib/python3.13/site-packages/transformer_lens/pretrained/weight_conversi\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33mons/\u001b[0m\u001b[1;33mllama.py\u001b[0m:\u001b[94m44\u001b[0m in \u001b[92mconvert_llama_weights\u001b[0m                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m41 \u001b[0m\u001b[2m      \u001b[0mstate_dict[\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mblocks.\u001b[0m\u001b[33m{\u001b[0ml\u001b[33m}\u001b[0m\u001b[33m.attn.\u001b[0m\u001b[33m{\u001b[0mgqa_uscore\u001b[33m}\u001b[0m\u001b[33mW_K\u001b[0m\u001b[33m\"\u001b[0m] = W_K                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m42 \u001b[0m\u001b[2m      \u001b[0mstate_dict[\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mblocks.\u001b[0m\u001b[33m{\u001b[0ml\u001b[33m}\u001b[0m\u001b[33m.attn.\u001b[0m\u001b[33m{\u001b[0mgqa_uscore\u001b[33m}\u001b[0m\u001b[33mW_V\u001b[0m\u001b[33m\"\u001b[0m] = W_V                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m43 \u001b[0m\u001b[2m      \u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m44 \u001b[2m      \u001b[0mstate_dict[\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mblocks.\u001b[0m\u001b[33m{\u001b[0ml\u001b[33m}\u001b[0m\u001b[33m.attn.b_Q\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[1;4mtorch.zeros(\u001b[0m                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m45 \u001b[0m\u001b[1;2;4m         \u001b[0m\u001b[1;4mcfg.n_heads, cfg.d_head, dtype=cfg.dtype, device=cfg.device\u001b[0m                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m46 \u001b[0m\u001b[1;2;4m      \u001b[0m\u001b[1;4m)\u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m47 \u001b[0m\u001b[2m      \u001b[0mstate_dict[\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mblocks.\u001b[0m\u001b[33m{\u001b[0ml\u001b[33m}\u001b[0m\u001b[33m.attn.\u001b[0m\u001b[33m{\u001b[0mgqa_uscore\u001b[33m}\u001b[0m\u001b[33mb_K\u001b[0m\u001b[33m\"\u001b[0m] = torch.zeros(                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/Users/brian/miniconda3/lib/python3.13/site-packages/torch/cuda/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m403\u001b[0m in \u001b[92m_lazy_init\u001b[0m    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 400 \u001b[0m\u001b[2m            \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mmultiprocessing, you must use the \u001b[0m\u001b[33m'\u001b[0m\u001b[33mspawn\u001b[0m\u001b[33m'\u001b[0m\u001b[33m start method\u001b[0m\u001b[33m\"\u001b[0m                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 401 \u001b[0m\u001b[2m         \u001b[0m)                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 402 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mhasattr\u001b[0m(torch._C, \u001b[33m\"\u001b[0m\u001b[33m_cuda_getDeviceCount\u001b[0m\u001b[33m\"\u001b[0m):                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 403 \u001b[2m         \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mAssertionError\u001b[0m\u001b[1;4m(\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mTorch not compiled with CUDA enabled\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m)\u001b[0m                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 404 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m _cudart \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 405 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mAssertionError\u001b[0m(                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 406 \u001b[0m\u001b[2m            \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mAssertionError: \u001b[0mTorch not compiled with CUDA enabled\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Using Lens to get activations and feed them to a layer 11 classifier\n",
    "\n",
    "# ============================================================================\n",
    "# Authenticate with Hugging Face (required for gated models like Llama)\n",
    "# ============================================================================\n",
    "# Load .env file from subspace-rerouting submodule\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file from the submodule directory\n",
    "submodule_path = os.path.abspath(os.path.join(os.getcwd(), '../../external/subspace-rerouting'))\n",
    "env_path = os.path.join(submodule_path, '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Authenticate with Hugging Face using token from .env\n",
    "try:\n",
    "    from huggingface_hub import login\n",
    "    \n",
    "    # Get token from environment (loaded from .env file)\n",
    "    hf_token = os.getenv('HF_TOKEN')\n",
    "    \n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\" Authenticated with Hugging Face using token from .env file\")\n",
    "    else:\n",
    "        print(\"  HF_TOKEN not found in .env file\")\n",
    "        print(\"   Please add HF_TOKEN=your_token_here to the .env file\")\n",
    "        print(\"   Get your token from: https://huggingface.co/settings/tokens\")\n",
    "        print(\"   Request access to model: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\")\n",
    "        # Try to login interactively as fallback\n",
    "        login()\n",
    "except ImportError:\n",
    "    print(\"  huggingface_hub not installed. Install with: pip install huggingface_hub\")\n",
    "    print(\"   Or set HF_TOKEN environment variable with your Hugging Face token\")\n",
    "\n",
    "# ============================================================================\n",
    "# Initialize Lens with a preset model (e.g., llama3.2_1b)\n",
    "# ============================================================================\n",
    "# Available presets: \"llama3.2_1b\", \"llama3.2_3b\", \"gemma2_2b\", \"qwen2.5_1.5b\"\n",
    "lens = Lens.from_preset(\"llama3.2_1b\")\n",
    "\n",
    "# Define your user message (no system prompt)\n",
    "user_message = \"What is the capital of France?\"\n",
    "\n",
    "# Apply chat template to format the message properly (without system prompt)\n",
    "formatted_prompt = lens.apply_chat_template(\n",
    "    user_message,\n",
    "    add_generation_prompt=True  # Add generation prompt\n",
    ")\n",
    "\n",
    "print(\"Formatted prompt:\")\n",
    "print(formatted_prompt)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Tokenize the formatted prompt\n",
    "tokens = lens.apply_chat_template(\n",
    "    user_message,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(f\"Tokenized input shape: {tokens.input_ids.shape}\")\n",
    "print(f\"Tokens: {lens.model.to_str_tokens(tokens.input_ids[0])}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run the model with cache to get activations\n",
    "# This returns logits and an ActivationCache containing all intermediate activations\n",
    "logits, cache = lens.model.run_with_cache(\n",
    "    tokens.input_ids,\n",
    "    names_filter=lambda name: \"resid_post\" in name  # Filter to get residual stream activations\n",
    ")\n",
    "\n",
    "# Extract activations from layer 11\n",
    "layer_idx = 11\n",
    "layer_activations = cache[\"resid_post\", layer_idx]\n",
    "print(f\"Layer {layer_idx} activations shape: {layer_activations.shape}\")\n",
    "print(f\"  - Batch size: {layer_activations.shape[0]}\")\n",
    "print(f\"  - Sequence length: {layer_activations.shape[1]}\")\n",
    "print(f\"  - Hidden dimension: {layer_activations.shape[2]}\")\n",
    "\n",
    "# Get activations at the last token position (often used for classification/analysis)\n",
    "last_token_activations = layer_activations[:, -1, :]  # Shape: (batch_size, hidden_dim)\n",
    "print(f\"\\nLast token activations shape: {last_token_activations.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load and use the layer 11 classifier\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Loading Layer 11 Classifier:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get the path to the subspace-rerouting submodule\n",
    "submodule_path = os.path.abspath(os.path.join(os.getcwd(), '../../external/subspace-rerouting'))\n",
    "classifier_dir = Path(submodule_path) / 'mlp_classifiers'\n",
    "\n",
    "# Find the most recent metadata file (or specify a specific one)\n",
    "metadata_files = sorted(classifier_dir.glob('*_metadata.pkl'))\n",
    "\n",
    "if len(metadata_files) == 0:\n",
    "    raise ValueError(f\"  No saved classifiers found in {classifier_dir}\")\n",
    "    \n",
    "# Load the most recent classifier set\n",
    "metadata_path = metadata_files[-1]\n",
    "print(f\"\\nLoading classifiers from: {metadata_path.name}\")\n",
    "\n",
    "# Load metadata\n",
    "metadata = joblib.load(metadata_path)\n",
    "print(f\"  Timestamp: {metadata['timestamp']}\")\n",
    "print(f\"  Layers trained: {metadata['layers_to_train']}\")\n",
    "print(f\"  Features per layer: {metadata['n_features']}\")\n",
    "\n",
    "# Extract base path from metadata filename\n",
    "base_name = metadata_path.stem.replace('_metadata', '')\n",
    "\n",
    "# Load layer 11 classifier\n",
    "layer_11_path = classifier_dir / f'{base_name}_layer_{layer_idx}.pkl'\n",
    "\n",
    "if not layer_11_path.exists():\n",
    "    raise ValueError(f\"  Layer {layer_idx} classifier not found at {layer_11_path}\")\n",
    "\n",
    "layer_11_data = joblib.load(layer_11_path)\n",
    "mlp_classifier = layer_11_data['mlp']\n",
    "scaler = layer_11_data['scaler']\n",
    "\n",
    "print(f\"\\n Loaded layer {layer_idx} classifier\")\n",
    "print(f\"  Test accuracy: {layer_11_data['test_accuracy']:.4f}\")\n",
    "print(f\"  Train accuracy: {layer_11_data['train_accuracy']:.4f}\")\n",
    "print(f\"  Label names: {layer_11_data.get('label_names', 'N/A')}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Feed activations to the classifier\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classifying Activations:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert activations to numpy and prepare for classification\n",
    "# Use last token activations (shape: batch_size, hidden_dim)\n",
    "activations_np = last_token_activations.detach().cpu().numpy()\n",
    "\n",
    "# Standardize using the same scaler used during training\n",
    "activations_scaled = scaler.transform(activations_np)\n",
    "\n",
    "# Get predictions\n",
    "predictions = mlp_classifier.predict(activations_scaled)\n",
    "prediction_proba = mlp_classifier.predict_proba(activations_scaled)\n",
    "\n",
    "print(f\"\\nInput activations shape: {activations_np.shape}\")\n",
    "print(f\"Scaled activations shape: {activations_scaled.shape}\")\n",
    "print(f\"\\nPredicted class: {predictions[0]}\")\n",
    "\n",
    "# Print probabilities for each class\n",
    "if 'label_names' in layer_11_data:\n",
    "    label_names = layer_11_data['label_names']\n",
    "    print(f\"\\nClass probabilities:\")\n",
    "    for i, (label, prob) in enumerate(zip(label_names, prediction_proba[0])):\n",
    "        print(f\"  {label}: {prob:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nClass probabilities:\")\n",
    "    for i, prob in enumerate(prediction_proba[0]):\n",
    "        print(f\"  Class {i}: {prob:.4f}\")\n",
    "\n",
    "print(f\"\\n Classification complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
